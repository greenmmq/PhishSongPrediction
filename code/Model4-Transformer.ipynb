{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSO58G6FGC19",
        "outputId": "bb42f111-74c4-4f2a-c3c5-a0114fc72889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# read-in cleaned data and parse to corpus and tokenizer\n",
        "# path = '../data/'\n",
        "path = '/content/'\n",
        "filepath = path + 'allphishsets.csv'\n",
        "\n",
        "df = pd.read_csv(filepath)\n",
        "df = df.sort_values(by=['showdate', 'set', 'position'],\n",
        "                    ascending=[True, True, True])\n",
        "\n",
        "df.loc[df['times_played'] <= 2, 'slug'] = 'wildcard'\n",
        "df.loc[df['times_played'] <= 2, 'times_played'] = 510\n",
        "\n",
        "songstring = df[['showdate', 'set', 'slug']].groupby(['showdate', 'set'])['slug']\\\n",
        "                                            .apply(lambda x: '|'.join(x)).reset_index()\n",
        "songstring['full'] = songstring.apply(lambda row: f\"set-{row['set']}|{row['slug']}\", axis=1)\n",
        "\n",
        "songstring = songstring[['showdate', 'full']].groupby(['showdate'])['full']\\\n",
        "                                             .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] += '|eos'\n",
        "\n",
        "corpus = [''.join(map(lambda s: s.replace('|', ' '), f))\n",
        "          for f in songstring['full']]\n",
        "\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "unique_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "def PrepareDataset(corpus: list, tokenizer: Tokenizer,\n",
        "                   n_context: int, batch_size: int, train_split: float):\n",
        "    \"\"\"\n",
        "    Prepares Datasets for training and validation data from Setlist data\n",
        "    Args:\n",
        "      corpus :: list :: full corpus of songs composed of setlists as sequences\n",
        "      tokenizer :: Tokenizer :: keras Tokenizer object trained on corpus\n",
        "      n_context :: int :: number of previous setlist to use as context for a\n",
        "                          given setlist\n",
        "      batch_size :: int :: batch size for datasets\n",
        "      train_split :: float :: values between 0 and 1, splits the data for\n",
        "                              training and validation\n",
        "    \"\"\"\n",
        "    max_seq_length = max([len(setlist.split(' ')) for setlist in corpus]) - 1\n",
        "\n",
        "    x_inputs = []\n",
        "    x_outputs = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        x_inputs.append(token_list[:-1])  #drop eos\n",
        "        x_outputs.append(token_list[1:])  #drop set-1\n",
        "\n",
        "    x_inputs = np.array(\n",
        "        pad_sequences(x_inputs, maxlen=max_seq_length, padding='post')\n",
        "    )\n",
        "    x_outputs = np.array(\n",
        "        pad_sequences(x_outputs, maxlen=max_seq_length, padding='post')\n",
        "    )\n",
        "\n",
        "    # last n shows as the context vector for each show\n",
        "    n = n_context\n",
        "    n_shows = []\n",
        "    for i in range(len(corpus[:-n])):\n",
        "        n_shows.append(' '.join(corpus[i:i+n]))\n",
        "\n",
        "    context = []\n",
        "    for line in n_shows:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        context.append(token_list)\n",
        "\n",
        "    max_context_length = max([len(x) for x in context])\n",
        "    x_context = np.array(\n",
        "        pad_sequences(context, maxlen=max_context_length, padding='post')\n",
        "    )\n",
        "\n",
        "    x_context = x_context[:-1]\n",
        "    x_inputs = x_inputs[n+1:]\n",
        "    x_outputs = x_outputs[n+1:]\n",
        "\n",
        "    buffer_size = len(x_context)\n",
        "    train_size = int(train_split*buffer_size)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(((x_context, x_inputs), x_outputs))\n",
        "    shuffled_data = dataset.shuffle(buffer_size)\n",
        "\n",
        "    train_data = dataset.take(train_size) \\\n",
        "                        .batch(batch_size) \\\n",
        "                        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    val_data = dataset.skip(train_size) \\\n",
        "                      .batch(batch_size) \\\n",
        "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_data, val_data"
      ],
      "metadata": {
        "id": "qoaZAlnwP9qf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "# classes for positional embedding and attention layers\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lscd8XJpPzyd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes for NN model architecture\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "# performance metrics\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "zHehKuMBGOUJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = PrepareDataset(\n",
        "    corpus=corpus,\n",
        "    tokenizer=tokenizer,\n",
        "    n_context=5,\n",
        "    batch_size=8,\n",
        "    train_split=0.8\n",
        ")\n",
        "\n",
        "d_model = 256\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9\n",
        ")\n",
        "\n",
        "num_layers = 4\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = .5\n",
        "epochs = 30\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=unique_words,\n",
        "    target_vocab_size=unique_words,\n",
        "    dropout_rate=dropout_rate\n",
        ")\n",
        "\n",
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy]\n",
        ")\n",
        "\n",
        "transformer.fit(train_data, validation_data=val_data, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJYGwRyGOLm",
        "outputId": "1e7d34d5-dcc4-427f-84ec-1b7e45616053"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "92/92 [==============================] - 75s 219ms/step - loss: 5.8365 - masked_accuracy: 0.0238 - val_loss: 5.5113 - val_masked_accuracy: 0.0612\n",
            "Epoch 2/30\n",
            "92/92 [==============================] - 10s 109ms/step - loss: 5.2271 - masked_accuracy: 0.0548 - val_loss: 5.3284 - val_masked_accuracy: 0.0935\n",
            "Epoch 3/30\n",
            "92/92 [==============================] - 10s 112ms/step - loss: 4.8925 - masked_accuracy: 0.0852 - val_loss: 5.1689 - val_masked_accuracy: 0.1044\n",
            "Epoch 4/30\n",
            "92/92 [==============================] - 10s 112ms/step - loss: 4.6259 - masked_accuracy: 0.1032 - val_loss: 5.1379 - val_masked_accuracy: 0.1009\n",
            "Epoch 5/30\n",
            "92/92 [==============================] - 11s 115ms/step - loss: 4.4281 - masked_accuracy: 0.1277 - val_loss: 5.0051 - val_masked_accuracy: 0.1422\n",
            "Epoch 6/30\n",
            "92/92 [==============================] - 10s 110ms/step - loss: 4.2135 - masked_accuracy: 0.1662 - val_loss: 4.9730 - val_masked_accuracy: 0.1553\n",
            "Epoch 7/30\n",
            "92/92 [==============================] - 10s 112ms/step - loss: 4.0486 - masked_accuracy: 0.1865 - val_loss: 4.9785 - val_masked_accuracy: 0.1635\n",
            "Epoch 8/30\n",
            "92/92 [==============================] - 10s 111ms/step - loss: 3.9561 - masked_accuracy: 0.1954 - val_loss: 4.9422 - val_masked_accuracy: 0.1632\n",
            "Epoch 9/30\n",
            "92/92 [==============================] - 10s 110ms/step - loss: 3.8583 - masked_accuracy: 0.2076 - val_loss: 4.9899 - val_masked_accuracy: 0.1646\n",
            "Epoch 10/30\n",
            "92/92 [==============================] - 10s 113ms/step - loss: 3.7825 - masked_accuracy: 0.2160 - val_loss: 4.9403 - val_masked_accuracy: 0.1660\n",
            "Epoch 11/30\n",
            "92/92 [==============================] - 10s 110ms/step - loss: 3.7144 - masked_accuracy: 0.2189 - val_loss: 5.0165 - val_masked_accuracy: 0.1678\n",
            "Epoch 12/30\n",
            "92/92 [==============================] - 10s 110ms/step - loss: 3.6494 - masked_accuracy: 0.2246 - val_loss: 5.1278 - val_masked_accuracy: 0.1708\n",
            "Epoch 13/30\n",
            "92/92 [==============================] - 10s 110ms/step - loss: 3.5941 - masked_accuracy: 0.2313 - val_loss: 5.1655 - val_masked_accuracy: 0.1666\n",
            "Epoch 14/30\n",
            "92/92 [==============================] - 10s 113ms/step - loss: 3.5409 - masked_accuracy: 0.2358 - val_loss: 5.2224 - val_masked_accuracy: 0.1686\n",
            "Epoch 15/30\n",
            "92/92 [==============================] - 10s 107ms/step - loss: 3.4911 - masked_accuracy: 0.2416 - val_loss: 5.2780 - val_masked_accuracy: 0.1665\n",
            "Epoch 16/30\n",
            "92/92 [==============================] - 11s 121ms/step - loss: 3.4721 - masked_accuracy: 0.2404 - val_loss: 5.3373 - val_masked_accuracy: 0.1684\n",
            "Epoch 17/30\n",
            "92/92 [==============================] - 10s 109ms/step - loss: 3.4182 - masked_accuracy: 0.2496 - val_loss: 5.4506 - val_masked_accuracy: 0.1720\n",
            "Epoch 18/30\n",
            "92/92 [==============================] - 10s 111ms/step - loss: 3.3943 - masked_accuracy: 0.2520 - val_loss: 5.4947 - val_masked_accuracy: 0.1741\n",
            "Epoch 19/30\n",
            "92/92 [==============================] - 10s 106ms/step - loss: 3.3421 - masked_accuracy: 0.2568 - val_loss: 5.5037 - val_masked_accuracy: 0.1699\n",
            "Epoch 20/30\n",
            "92/92 [==============================] - 10s 109ms/step - loss: 3.3075 - masked_accuracy: 0.2581 - val_loss: 5.6612 - val_masked_accuracy: 0.1745\n",
            "Epoch 21/30\n",
            "92/92 [==============================] - 10s 110ms/step - loss: 3.2986 - masked_accuracy: 0.2579 - val_loss: 5.6330 - val_masked_accuracy: 0.1705\n",
            "Epoch 22/30\n",
            "92/92 [==============================] - 10s 110ms/step - loss: 3.2736 - masked_accuracy: 0.2617 - val_loss: 5.7442 - val_masked_accuracy: 0.1701\n",
            "Epoch 23/30\n",
            "92/92 [==============================] - 10s 106ms/step - loss: 3.2452 - masked_accuracy: 0.2664 - val_loss: 5.8373 - val_masked_accuracy: 0.1665\n",
            "Epoch 24/30\n",
            "92/92 [==============================] - 10s 110ms/step - loss: 3.2120 - masked_accuracy: 0.2669 - val_loss: 5.9416 - val_masked_accuracy: 0.1672\n",
            "Epoch 25/30\n",
            "92/92 [==============================] - 10s 109ms/step - loss: 3.2088 - masked_accuracy: 0.2662 - val_loss: 6.0193 - val_masked_accuracy: 0.1666\n",
            "Epoch 26/30\n",
            "92/92 [==============================] - 10s 109ms/step - loss: 3.1744 - masked_accuracy: 0.2685 - val_loss: 6.0117 - val_masked_accuracy: 0.1634\n",
            "Epoch 27/30\n",
            "92/92 [==============================] - 10s 106ms/step - loss: 3.2086 - masked_accuracy: 0.2648 - val_loss: 5.9012 - val_masked_accuracy: 0.1651\n",
            "Epoch 28/30\n",
            "92/92 [==============================] - 10s 109ms/step - loss: 3.1763 - masked_accuracy: 0.2701 - val_loss: 6.0282 - val_masked_accuracy: 0.1727\n",
            "Epoch 29/30\n",
            "92/92 [==============================] - 10s 109ms/step - loss: 3.1402 - masked_accuracy: 0.2710 - val_loss: 5.9977 - val_masked_accuracy: 0.1705\n",
            "Epoch 30/30\n",
            "92/92 [==============================] - 10s 111ms/step - loss: 3.1084 - masked_accuracy: 0.2784 - val_loss: 6.0730 - val_masked_accuracy: 0.1684\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ded1df665c0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model the output\n",
        "\n",
        "for (context, input), labels in val_data.take(3):\n",
        "  break\n",
        "\n",
        "encoder_input = tf.reshape(context[0], (1, len(context[0])))\n",
        "\n",
        "output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "output_array = output_array.write(0, 1)\n",
        "\n",
        "for i in tf.range(25):\n",
        "    output = tf.reshape(output_array.stack(), (1, len(output_array.stack())))\n",
        "    predictions = transformer([encoder_input, output], training=False)\n",
        "\n",
        "    # Select the last token from the `seq_len` dimension.\n",
        "    predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)[0][0]\n",
        "\n",
        "    # Concatenate the `predicted_id` to the output which is given to the\n",
        "    # decoder as its input.\n",
        "    output_array = output_array.write(i+1, predicted_id)\n",
        "\n",
        "    if predicted_id == 4:\n",
        "        break\n",
        "\n",
        "[tokenizer.index_word[s] for s in output_array.stack().numpy()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL58IFvsGN2X",
        "outputId": "eb4a8f4a-d84e-4ceb-8b75-97dfe11f8c0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set-1',\n",
              " 'punch-you-in-the-eye',\n",
              " 'gumbo',\n",
              " 'birds-of-a-feather',\n",
              " 'guyute',\n",
              " 'my-soul',\n",
              " 'set-2',\n",
              " 'tweezer',\n",
              " 'have-mercy',\n",
              " 'taste',\n",
              " 'the-moma-dance',\n",
              " 'mountains-in-the-mist',\n",
              " 'you-enjoy-myself',\n",
              " 'set-e']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in labels[0].numpy() if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1uDvSm5TIq8",
        "outputId": "53ec7d3c-9abf-404c-a2c4-820164a82c40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fuego',\n",
              " 'my-soul',\n",
              " 'back-on-the-train',\n",
              " '555',\n",
              " 'dog-faced-boy',\n",
              " 'fuck-your-face',\n",
              " 'horn',\n",
              " 'frankie-says',\n",
              " 'my-friend-my-friend',\n",
              " 'roses-are-free',\n",
              " 'roggae',\n",
              " 'birds-of-a-feather',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'possum',\n",
              " 'crosseyed-and-painless',\n",
              " 'light',\n",
              " 'the-dogs',\n",
              " 'lengthwise',\n",
              " 'twist',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'harry-hood',\n",
              " 'golgi-apparatus',\n",
              " 'backwards-down-the-number-line',\n",
              " 'set-e',\n",
              " 'waiting-all-night',\n",
              " 'sing-monica',\n",
              " 'the-star-spangled-banner',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in encoder_input.numpy()[0] if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqBo5_GoGNmR",
        "outputId": "09c1c3dd-5ba6-4b73-a6a8-9398f4bc66f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set-1',\n",
              " '46-days',\n",
              " 'tube',\n",
              " 'train-song',\n",
              " 'ghost',\n",
              " 'sparkle',\n",
              " 'sample-in-a-jar',\n",
              " 'divided-sky',\n",
              " 'the-line',\n",
              " 'its-ice',\n",
              " 'kill-devil-falls',\n",
              " 'bathtub-gin',\n",
              " 'set-2',\n",
              " '555',\n",
              " 'backwards-down-the-number-line',\n",
              " 'down-with-disease',\n",
              " 'fuego',\n",
              " 'twist',\n",
              " 'bouncing-around-the-room',\n",
              " 'david-bowie',\n",
              " 'character-zero',\n",
              " 'set-e',\n",
              " 'harry-hood',\n",
              " 'grind',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'devotion-to-a-dream',\n",
              " 'acdc-bag',\n",
              " 'my-sweet-one',\n",
              " 'the-moma-dance',\n",
              " 'halleys-comet',\n",
              " 'funky-bitch',\n",
              " 'wolfmans-brother',\n",
              " 'destiny-unbound',\n",
              " 'timber-jerry-the-mule',\n",
              " 'tela',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'free',\n",
              " 'golden-age',\n",
              " 'gotta-jibboo',\n",
              " 'carini',\n",
              " 'piper',\n",
              " 'prince-caspian',\n",
              " 'tweezer',\n",
              " 'rock-and-roll',\n",
              " 'you-enjoy-myself',\n",
              " 'set-e',\n",
              " 'suzy-greenberg',\n",
              " 'tweezer-reprise',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'walfredo',\n",
              " 'ocelot',\n",
              " 'camel-walk',\n",
              " 'axilla',\n",
              " 'rift',\n",
              " '555',\n",
              " 'maze',\n",
              " 'brian-and-robert',\n",
              " 'stash',\n",
              " 'party-time',\n",
              " '46-days',\n",
              " 'set-2',\n",
              " 'sand',\n",
              " 'birds-of-a-feather',\n",
              " 'waiting-all-night',\n",
              " 'ghost',\n",
              " 'bug',\n",
              " 'seven-below',\n",
              " 'i-didnt-know',\n",
              " 'chalk-dust-torture',\n",
              " 'also-sprach-zarathustra',\n",
              " 'slave-to-the-traffic-light',\n",
              " 'set-e',\n",
              " 'wildcard',\n",
              " 'winterqueen',\n",
              " 'a-day-in-the-life',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'crowd-control',\n",
              " 'mikes-song',\n",
              " 'i-am-hydrogen',\n",
              " 'weekapaug-groove',\n",
              " 'wingsuit',\n",
              " 'water-in-the-sky',\n",
              " 'plasma',\n",
              " 'halfway-to-the-moon',\n",
              " 'poor-heart',\n",
              " 'gumbo',\n",
              " 'sanity',\n",
              " 'run-like-an-antelope',\n",
              " 'set-2',\n",
              " 'kill-devil-falls',\n",
              " 'mountains-in-the-mist',\n",
              " 'fuego',\n",
              " 'julius',\n",
              " 'twist',\n",
              " 'runaway-jim',\n",
              " 'harry-hood',\n",
              " 'set-e',\n",
              " 'loving-cup',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'stealing-time-from-the-faulty-plan',\n",
              " 'the-moma-dance',\n",
              " 'free',\n",
              " 'back-on-the-train',\n",
              " 'yarmouth-road',\n",
              " 'strange-design',\n",
              " 'taste',\n",
              " 'the-wedge',\n",
              " 'the-line',\n",
              " 'wolfmans-brother',\n",
              " 'set-2',\n",
              " 'first-tube',\n",
              " 'down-with-disease',\n",
              " 'theme-from-the-bottom',\n",
              " 'split-open-and-melt',\n",
              " 'heavy-things',\n",
              " 'light',\n",
              " 'possum',\n",
              " 'set-e',\n",
              " 'contact',\n",
              " 'meatstick',\n",
              " 'character-zero',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate a sequence based on a given seed\n",
        "def generate_sequence(seed_sequence, max_length=25):\n",
        "    encoder_input = tf.constant([tokenizer.word_index[word] for word in seed_sequence], dtype=tf.int64)\n",
        "    encoder_input = tf.reshape(encoder_input, (1, len(encoder_input)))\n",
        "\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, tokenizer.word_index[seed_sequence[-1]])\n",
        "\n",
        "    unique_tokens = set([tokenizer.word_index[word] for word in seed_sequence])\n",
        "\n",
        "    i = 0\n",
        "    while i < max_length:\n",
        "        output = tf.reshape(output_array.stack(), (1, len(output_array.stack())))\n",
        "        predictions = transformer([encoder_input, output], training=False)\n",
        "\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)[0][0]\n",
        "\n",
        "        # Convert the scalar TensorFlow tensor to a tuple\n",
        "        predicted_id_tuple = (predicted_id.numpy(),)\n",
        "\n",
        "        # Check for duplicates\n",
        "        if predicted_id_tuple in unique_tokens :\n",
        "\n",
        "            output_array = output_array.write(i+1, predicted_id)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        unique_tokens.add(predicted_id_tuple)\n",
        "\n",
        "        output_array = output_array.write(i+1, predicted_id)\n",
        "\n",
        "        unique_tokens = set(unique_tokens)\n",
        "        #if predicted_id == 4:\n",
        "        #    output_array = output_array.write(i+1, predicted_id)\n",
        "        #    i += 1\n",
        "        #    continue;\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    predicted_sequence = [tokenizer.index_word[s] for s in output_array.stack().numpy()]\n",
        "\n",
        "    return predicted_sequence\n",
        "\n",
        "# Example usage:\n",
        "#org_list = corpus[-1].split()\n",
        "org_list = [tokenizer.index_word[s] for s in labels[6].numpy() if s != 0]\n",
        "org_list = ['farmhouse', 'first-tube', 'twist', 'divided-sky', 'ginseng-sullivan', 'carini', 'whats-the-use', 'wildcard', 'set-2', 'down-with-disease', 'the-moma-dance', 'piper', 'fee', 'gotta-jibboo', 'saw-it-again', 'split-open-and-melt', 'cavern', 'david-bowie', 'set-e', 'the-squirming-coil', 'eos']\n",
        "seed_sequence = org_list[:3]\n",
        "max_length = len(org_list) - len(seed_sequence)\n",
        "predicted_sequence = generate_sequence(seed_sequence, max_length=max_length)\n",
        "\n",
        "print(\"Original Sequence:\", org_list)\n",
        "print(\"Seed Sequence:\", seed_sequence)\n",
        "print(\"Predicted Sequence:\", (predicted_sequence[len(seed_sequence):]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_owwTjMzQlqS",
        "outputId": "87789d5f-23f4-4d2d-b9af-57a055c02d5e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sequence: ['farmhouse', 'first-tube', 'twist', 'divided-sky', 'ginseng-sullivan', 'carini', 'whats-the-use', 'wildcard', 'set-2', 'down-with-disease', 'the-moma-dance', 'piper', 'fee', 'gotta-jibboo', 'saw-it-again', 'split-open-and-melt', 'cavern', 'david-bowie', 'set-e', 'the-squirming-coil', 'eos']\n",
            "Seed Sequence: ['farmhouse', 'first-tube', 'twist']\n",
            "Predicted Sequence: ['limb-by-limb', 'farmhouse', 'water-in-the-sky', 'limb-by-limb', 'train-song', 'water-in-the-sky', 'character-zero', 'set-2', 'runaway-jim', 'the-moma-dance', 'piper', 'prince-caspian', 'you-enjoy-myself', 'set-e', 'wildcard', 'eos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming org_list, seed_sequence, stopword_list, and predicted_sequence are lists of songs\n",
        "\n",
        "stopword_list = {}\n",
        "\n",
        "\n",
        "# Convert the lists to sets\n",
        "org_set = set(org_list)\n",
        "seed_set = set(seed_sequence)\n",
        "not_in_seed_sequence = org_set - seed_set\n",
        "filtered_songs = [song.strip() for song in not_in_seed_sequence if song not in stopword_list]\n",
        "predicted_sequence = [song.strip() for song in predicted_sequence[len(seed_sequence):] if song not in stopword_list]\n",
        "\n",
        "# Count the number of matching songs\n",
        "matching_songs = [song for song in filtered_songs if song in predicted_sequence]\n",
        "num_matching_songs = len(matching_songs)\n",
        "\n",
        "# Calculate the percentage of matching songs\n",
        "percentage_matching = (num_matching_songs / len(filtered_songs)) * 100\n",
        "\n",
        "print(\"Matching Songs in Predicted Sequence:\")\n",
        "print(matching_songs)\n",
        "print(\"Number of Matching Songs:\", num_matching_songs)\n",
        "print(\"Percentage of Matching Songs:\", percentage_matching)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVlLpOTbQzo7",
        "outputId": "fa7fd08d-5b67-428a-f020-0760d38343f1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching Songs in Predicted Sequence:\n",
            "['set-e', 'wildcard', 'piper', 'the-moma-dance', 'eos', 'set-2']\n",
            "Number of Matching Songs: 6\n",
            "Percentage of Matching Songs: 33.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOzGteVPV1WF"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}
