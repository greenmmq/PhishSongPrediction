{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSO58G6FGC19",
        "outputId": "95e9acba-47b6-411f-b762-fae809497ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# read-in cleaned data and parse to corpus and tokenizer\n",
        "# path = '../data/'\n",
        "path = '/content/'\n",
        "filepath = path + 'allphishsets.csv'\n",
        "\n",
        "df = pd.read_csv(filepath)\n",
        "df = df.sort_values(by=['showdate', 'set', 'position'],\n",
        "                    ascending=[True, True, True])\n",
        "\n",
        "df.loc[df['times_played'] <= 2, 'slug'] = 'wildcard'\n",
        "df.loc[df['times_played'] <= 2, 'times_played'] = 510\n",
        "\n",
        "songstring = df[['showdate', 'set', 'slug']].groupby(['showdate', 'set'])['slug']\\\n",
        "                                            .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] = songstring.apply(lambda row: f\"set-{row['set']}|{row['slug']}\", axis=1)\n",
        "\n",
        "songstring = songstring[['showdate', 'full']].groupby(['showdate'])['full']\\\n",
        "                                             .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] += '|eos'\n",
        "\n",
        "songstring = songstring[songstring['showdate']>'1990-01-01']\n",
        "\n",
        "corpus = [''.join(map(lambda s: s.replace('|', ' '), f))\n",
        "          for f in songstring['full']]\n",
        "\n",
        "corpus = ' '.join(corpus).split(' ')\n",
        "\n",
        "def PrepareDataset(corpus: list, n: int,\n",
        "                   batch_size: int, train_split: float):\n",
        "    \"\"\"\n",
        "    Prepares Datasets for training and validation data from Setlist data\n",
        "    Args:\n",
        "      corpus :: list :: full corpus of songs composed of song sequences\n",
        "      n :: int :: sequence length to trim\n",
        "      batch_size :: int :: batch size for datasets\n",
        "      train_split :: float :: values between 0 and 1, splits the data for\n",
        "                              training and validation\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    for i in range(len(corpus)-n):\n",
        "        texts.append(' '.join(corpus[i:i+n]))\n",
        "\n",
        "    texts = texts[::n]\n",
        "\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    x_inputs = []\n",
        "    x_outputs = []\n",
        "    for line in texts:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        x_inputs.append(token_list[:-1])  #drop last song\n",
        "        x_outputs.append(token_list[1:])  #drop first song\n",
        "\n",
        "    x_inputs = np.array(x_inputs)\n",
        "    x_outputs = np.array(x_outputs)\n",
        "\n",
        "    buffer_size = len(x_inputs)\n",
        "    train_size = int(train_split*buffer_size)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_inputs, x_outputs))\n",
        "    shuffled_data = dataset.shuffle(buffer_size)\n",
        "\n",
        "    train_data = dataset.take(train_size) \\\n",
        "                        .batch(batch_size) \\\n",
        "                        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    val_data = dataset.skip(train_size) \\\n",
        "                      .batch(batch_size) \\\n",
        "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_data, val_data, tokenizer"
      ],
      "metadata": {
        "id": "qoaZAlnwP9qf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "# classes for positional embedding and attention layers\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lscd8XJpPzyd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes for NN model architecture\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.causal_self_attention(x=x)\n",
        "\n",
        "    x = self.add([x, y])\n",
        "    x = self.layer_norm(x)\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x)\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPT(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "\n",
        "    x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "    try:\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "# performance metrics\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "zHehKuMBGOUJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, tokenizer= PrepareDataset(\n",
        "    corpus=corpus,\n",
        "    n=125,\n",
        "    batch_size=32,\n",
        "    train_split=0.8\n",
        ")\n",
        "\n",
        "unique_words = len(tokenizer.word_index) + 1\n",
        "d_model = 128\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9\n",
        ")\n",
        "\n",
        "num_layers = 8\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "epochs = 100\n",
        "\n",
        "gpt = GPT(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=unique_words,\n",
        "    target_vocab_size=unique_words,\n",
        "    dropout_rate=dropout_rate\n",
        ")\n",
        "\n",
        "gpt.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy]\n",
        ")\n",
        "\n",
        "gpt.fit(train_data, validation_data=val_data, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJYGwRyGOLm",
        "outputId": "bac6e147-9f04-482f-ed2e-ae04613b7633"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 36s 484ms/step - loss: 6.4148 - masked_accuracy: 0.0026 - val_loss: 6.4264 - val_masked_accuracy: 0.0017\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 1s 160ms/step - loss: 6.4006 - masked_accuracy: 0.0027 - val_loss: 6.3987 - val_masked_accuracy: 0.0026\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 1s 178ms/step - loss: 6.3558 - masked_accuracy: 0.0035 - val_loss: 6.3520 - val_masked_accuracy: 0.0049\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 1s 188ms/step - loss: 6.2968 - masked_accuracy: 0.0064 - val_loss: 6.2894 - val_masked_accuracy: 0.0114\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 6.2244 - masked_accuracy: 0.0111 - val_loss: 6.2165 - val_masked_accuracy: 0.0204\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 2s 224ms/step - loss: 6.1411 - masked_accuracy: 0.0200 - val_loss: 6.1407 - val_masked_accuracy: 0.0298\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 6.0565 - masked_accuracy: 0.0292 - val_loss: 6.0694 - val_masked_accuracy: 0.0477\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 1s 163ms/step - loss: 5.9730 - masked_accuracy: 0.0433 - val_loss: 6.0063 - val_masked_accuracy: 0.0639\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 1s 176ms/step - loss: 5.9006 - masked_accuracy: 0.0543 - val_loss: 5.9516 - val_masked_accuracy: 0.0679\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 1s 176ms/step - loss: 5.8369 - masked_accuracy: 0.0594 - val_loss: 5.9044 - val_masked_accuracy: 0.0673\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 1s 167ms/step - loss: 5.7838 - masked_accuracy: 0.0609 - val_loss: 5.8636 - val_masked_accuracy: 0.0684\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: 5.7349 - masked_accuracy: 0.0649 - val_loss: 5.8282 - val_masked_accuracy: 0.0706\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: 5.6889 - masked_accuracy: 0.0675 - val_loss: 5.7952 - val_masked_accuracy: 0.0723\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 1s 165ms/step - loss: 5.6447 - masked_accuracy: 0.0691 - val_loss: 5.7605 - val_masked_accuracy: 0.0742\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 5.5972 - masked_accuracy: 0.0716 - val_loss: 5.7221 - val_masked_accuracy: 0.0777\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 1s 165ms/step - loss: 5.5490 - masked_accuracy: 0.0746 - val_loss: 5.6821 - val_masked_accuracy: 0.0814\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 5.5008 - masked_accuracy: 0.0772 - val_loss: 5.6466 - val_masked_accuracy: 0.0837\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 5.4523 - masked_accuracy: 0.0792 - val_loss: 5.6186 - val_masked_accuracy: 0.0850\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 5.4103 - masked_accuracy: 0.0804 - val_loss: 5.5951 - val_masked_accuracy: 0.0876\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 5.3715 - masked_accuracy: 0.0833 - val_loss: 5.5750 - val_masked_accuracy: 0.0890\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 5.3339 - masked_accuracy: 0.0838 - val_loss: 5.5553 - val_masked_accuracy: 0.0891\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 1s 145ms/step - loss: 5.2996 - masked_accuracy: 0.0840 - val_loss: 5.5387 - val_masked_accuracy: 0.0911\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 5.2635 - masked_accuracy: 0.0855 - val_loss: 5.5222 - val_masked_accuracy: 0.0907\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 5.2259 - masked_accuracy: 0.0881 - val_loss: 5.5085 - val_masked_accuracy: 0.0923\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 5.1912 - masked_accuracy: 0.0879 - val_loss: 5.4930 - val_masked_accuracy: 0.0953\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 5.1552 - masked_accuracy: 0.0918 - val_loss: 5.4774 - val_masked_accuracy: 0.0962\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 5.1132 - masked_accuracy: 0.0965 - val_loss: 5.4634 - val_masked_accuracy: 0.0974\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 5.0749 - masked_accuracy: 0.1009 - val_loss: 5.4483 - val_masked_accuracy: 0.0974\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 5.0338 - masked_accuracy: 0.1047 - val_loss: 5.4333 - val_masked_accuracy: 0.1000\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 4.9922 - masked_accuracy: 0.1111 - val_loss: 5.4206 - val_masked_accuracy: 0.1019\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 4.9507 - masked_accuracy: 0.1166 - val_loss: 5.4111 - val_masked_accuracy: 0.1041\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 4.9134 - masked_accuracy: 0.1206 - val_loss: 5.3989 - val_masked_accuracy: 0.1063\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 1s 145ms/step - loss: 4.8706 - masked_accuracy: 0.1254 - val_loss: 5.3879 - val_masked_accuracy: 0.1081\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 4.8361 - masked_accuracy: 0.1288 - val_loss: 5.3762 - val_masked_accuracy: 0.1098\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 4.8001 - masked_accuracy: 0.1323 - val_loss: 5.3692 - val_masked_accuracy: 0.1109\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 4.7670 - masked_accuracy: 0.1347 - val_loss: 5.3614 - val_masked_accuracy: 0.1126\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 4.7345 - masked_accuracy: 0.1389 - val_loss: 5.3563 - val_masked_accuracy: 0.1126\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 4.7016 - masked_accuracy: 0.1392 - val_loss: 5.3506 - val_masked_accuracy: 0.1137\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 4.6719 - masked_accuracy: 0.1431 - val_loss: 5.3516 - val_masked_accuracy: 0.1132\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 4.6383 - masked_accuracy: 0.1474 - val_loss: 5.3464 - val_masked_accuracy: 0.1155\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 4.6109 - masked_accuracy: 0.1536 - val_loss: 5.3438 - val_masked_accuracy: 0.1163\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 4.5878 - masked_accuracy: 0.1555 - val_loss: 5.3316 - val_masked_accuracy: 0.1235\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 4.5657 - masked_accuracy: 0.1593 - val_loss: 5.3270 - val_masked_accuracy: 0.1284\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 4.5507 - masked_accuracy: 0.1598 - val_loss: 5.3333 - val_masked_accuracy: 0.1265\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 4.5166 - masked_accuracy: 0.1632 - val_loss: 5.3342 - val_masked_accuracy: 0.1221\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 4.4884 - masked_accuracy: 0.1701 - val_loss: 5.3108 - val_masked_accuracy: 0.1341\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 1s 149ms/step - loss: 4.4644 - masked_accuracy: 0.1728 - val_loss: 5.3016 - val_masked_accuracy: 0.1362\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 4.4368 - masked_accuracy: 0.1771 - val_loss: 5.2918 - val_masked_accuracy: 0.1372\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 4.4157 - masked_accuracy: 0.1802 - val_loss: 5.2810 - val_masked_accuracy: 0.1421\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 4.3863 - masked_accuracy: 0.1850 - val_loss: 5.2601 - val_masked_accuracy: 0.1436\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 1s 136ms/step - loss: 4.3664 - masked_accuracy: 0.1881 - val_loss: 5.2432 - val_masked_accuracy: 0.1487\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 4.3401 - masked_accuracy: 0.1931 - val_loss: 5.2275 - val_masked_accuracy: 0.1524\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 4.3200 - masked_accuracy: 0.1923 - val_loss: 5.2204 - val_masked_accuracy: 0.1505\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 1s 136ms/step - loss: 4.2996 - masked_accuracy: 0.1951 - val_loss: 5.2369 - val_masked_accuracy: 0.1413\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 4.2697 - masked_accuracy: 0.1981 - val_loss: 5.1979 - val_masked_accuracy: 0.1529\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 1s 136ms/step - loss: 4.2612 - masked_accuracy: 0.2006 - val_loss: 5.2139 - val_masked_accuracy: 0.1581\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 1s 145ms/step - loss: 4.2278 - masked_accuracy: 0.2020 - val_loss: 5.1724 - val_masked_accuracy: 0.1630\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 4.1866 - masked_accuracy: 0.2113 - val_loss: 5.1286 - val_masked_accuracy: 0.1659\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 1s 149ms/step - loss: 4.1781 - masked_accuracy: 0.2127 - val_loss: 5.1097 - val_masked_accuracy: 0.1656\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 4.1530 - masked_accuracy: 0.2160 - val_loss: 5.1091 - val_masked_accuracy: 0.1684\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 4.1382 - masked_accuracy: 0.2173 - val_loss: 5.1515 - val_masked_accuracy: 0.1717\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 4.1044 - masked_accuracy: 0.2204 - val_loss: 5.1789 - val_masked_accuracy: 0.1672\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 4.0819 - masked_accuracy: 0.2189 - val_loss: 5.1043 - val_masked_accuracy: 0.1695\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 4.0762 - masked_accuracy: 0.2219 - val_loss: 5.1170 - val_masked_accuracy: 0.1700\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 4.0592 - masked_accuracy: 0.2207 - val_loss: 5.1217 - val_masked_accuracy: 0.1802\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 1s 136ms/step - loss: 4.0060 - masked_accuracy: 0.2292 - val_loss: 5.0927 - val_masked_accuracy: 0.1748\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 1s 136ms/step - loss: 3.9886 - masked_accuracy: 0.2308 - val_loss: 5.0601 - val_masked_accuracy: 0.1775\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 1s 135ms/step - loss: 4.0023 - masked_accuracy: 0.2302 - val_loss: 5.0359 - val_masked_accuracy: 0.1804\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 3.9722 - masked_accuracy: 0.2310 - val_loss: 5.0537 - val_masked_accuracy: 0.1829\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 3.9424 - masked_accuracy: 0.2327 - val_loss: 5.1093 - val_masked_accuracy: 0.1768\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 1s 147ms/step - loss: 3.9347 - masked_accuracy: 0.2349 - val_loss: 5.0969 - val_masked_accuracy: 0.1737\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 1s 143ms/step - loss: 3.9059 - masked_accuracy: 0.2348 - val_loss: 5.0335 - val_masked_accuracy: 0.1815\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 1s 136ms/step - loss: 3.9483 - masked_accuracy: 0.2331 - val_loss: 5.0943 - val_masked_accuracy: 0.1717\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.8582 - masked_accuracy: 0.2419 - val_loss: 5.0309 - val_masked_accuracy: 0.1852\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 1s 134ms/step - loss: 3.8386 - masked_accuracy: 0.2428 - val_loss: 5.0380 - val_masked_accuracy: 0.1831\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 3.8093 - masked_accuracy: 0.2454 - val_loss: 5.0230 - val_masked_accuracy: 0.1862\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.8050 - masked_accuracy: 0.2461 - val_loss: 4.9995 - val_masked_accuracy: 0.1862\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 1s 164ms/step - loss: 3.7882 - masked_accuracy: 0.2484 - val_loss: 4.9979 - val_masked_accuracy: 0.1854\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 1s 160ms/step - loss: 3.7700 - masked_accuracy: 0.2495 - val_loss: 5.0131 - val_masked_accuracy: 0.1845\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 1s 169ms/step - loss: 3.7657 - masked_accuracy: 0.2481 - val_loss: 5.0251 - val_masked_accuracy: 0.1844\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 3.8055 - masked_accuracy: 0.2439 - val_loss: 5.1230 - val_masked_accuracy: 0.1848\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 1s 149ms/step - loss: 3.7695 - masked_accuracy: 0.2440 - val_loss: 5.1684 - val_masked_accuracy: 0.1818\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 3.7582 - masked_accuracy: 0.2425 - val_loss: 5.0389 - val_masked_accuracy: 0.1803\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.8092 - masked_accuracy: 0.2456 - val_loss: 5.0955 - val_masked_accuracy: 0.1863\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.7053 - masked_accuracy: 0.2534 - val_loss: 5.1246 - val_masked_accuracy: 0.1851\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 3.6792 - masked_accuracy: 0.2524 - val_loss: 5.0482 - val_masked_accuracy: 0.1811\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 3.7072 - masked_accuracy: 0.2539 - val_loss: 5.0535 - val_masked_accuracy: 0.1871\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 3.6814 - masked_accuracy: 0.2528 - val_loss: 5.1718 - val_masked_accuracy: 0.1846\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.6528 - masked_accuracy: 0.2564 - val_loss: 5.0797 - val_masked_accuracy: 0.1846\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.6785 - masked_accuracy: 0.2530 - val_loss: 5.0817 - val_masked_accuracy: 0.1821\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 3.6475 - masked_accuracy: 0.2581 - val_loss: 5.1638 - val_masked_accuracy: 0.1825\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 3.6187 - masked_accuracy: 0.2597 - val_loss: 5.1058 - val_masked_accuracy: 0.1875\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 3.6087 - masked_accuracy: 0.2621 - val_loss: 5.1011 - val_masked_accuracy: 0.1864\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 3.6086 - masked_accuracy: 0.2625 - val_loss: 5.1281 - val_masked_accuracy: 0.1846\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 3.5640 - masked_accuracy: 0.2647 - val_loss: 5.1230 - val_masked_accuracy: 0.1862\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.5465 - masked_accuracy: 0.2657 - val_loss: 5.0954 - val_masked_accuracy: 0.1847\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.5287 - masked_accuracy: 0.2663 - val_loss: 5.1237 - val_masked_accuracy: 0.1866\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 3.5032 - masked_accuracy: 0.2729 - val_loss: 5.1091 - val_masked_accuracy: 0.1838\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.4954 - masked_accuracy: 0.2740 - val_loss: 5.1231 - val_masked_accuracy: 0.1827\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 1s 136ms/step - loss: 3.4862 - masked_accuracy: 0.2728 - val_loss: 5.1700 - val_masked_accuracy: 0.1895\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7db6bb03f4c0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model the output\n",
        "test_idx = 1\n",
        "\n",
        "for input, labels in val_data.take(test_idx):\n",
        "    break\n",
        "\n",
        "test_labels = labels[0][12:-13]\n",
        "\n",
        "output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "\n",
        "for i in range(len(test_labels.numpy())):\n",
        "    output_array = output_array.write(i, test_labels.numpy()[i])\n",
        "\n",
        "# output_array = output_array.write(0, 1)\n",
        "# output_array = output_array.write(1, 100)\n",
        "\n",
        "for i in tf.range(len(output_array.stack().numpy()), len(input[0])*2):\n",
        "    output = tf.reshape(output_array.stack(), (1, len(output_array.stack())))\n",
        "    predictions = gpt(output, training=False)\n",
        "\n",
        "    # Select the last token from the `seq_len` dimension.\n",
        "    predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)[0][0]\n",
        "\n",
        "    # Concatenate the `predicted_id` to the output which is given to the\n",
        "    # decoder as its input.\n",
        "    output_array = output_array.write(i+1, predicted_id)\n",
        "\n",
        "    if predicted_id == 4:\n",
        "        break\n",
        "\n",
        "[tokenizer.index_word[s] for s in output_array.stack().numpy() if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL58IFvsGN2X",
        "outputId": "cd801bff-c33b-492a-db9f-aa20b02a23fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['funky-bitch',\n",
              " 'maze',\n",
              " 'ocelot',\n",
              " 'sparkle',\n",
              " 'cavern',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'carini',\n",
              " 'ghost',\n",
              " 'mikes-song',\n",
              " 'simple',\n",
              " 'joy',\n",
              " 'weekapaug-groove',\n",
              " 'julius',\n",
              " 'sand',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'you-enjoy-myself',\n",
              " 'set-e',\n",
              " 'quinn-the-eskimo-the-mighty-quinn',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'my-soul',\n",
              " 'bathtub-gin',\n",
              " '555',\n",
              " 'pebbles-and-marbles',\n",
              " 'the-line',\n",
              " 'vultures',\n",
              " 'fast-enough-for-you',\n",
              " 'back-on-the-train',\n",
              " 'taste',\n",
              " 'gumbo',\n",
              " 'halfway-to-the-moon',\n",
              " 'stealing-time-from-the-faulty-plan',\n",
              " 'suzy-greenberg',\n",
              " 'set-2',\n",
              " 'chalk-dust-torture',\n",
              " 'scents-and-subtle-sounds',\n",
              " 'twist',\n",
              " 'fuego',\n",
              " 'the-wedge',\n",
              " 'light',\n",
              " 'harry-hood',\n",
              " 'first-tube',\n",
              " 'set-e',\n",
              " 'fluffhead',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'llama',\n",
              " 'undermind',\n",
              " 'stash',\n",
              " 'halfway-to-the-moon',\n",
              " 'i-didnt-know',\n",
              " 'nellie-kane',\n",
              " 'guyute',\n",
              " 'the-line',\n",
              " 'ocelot',\n",
              " 'no-quarter',\n",
              " 'ha-ha-ha',\n",
              " 'suzy-greenberg',\n",
              " 'set-2',\n",
              " '46-days',\n",
              " 'back-on-the-train',\n",
              " 'simple',\n",
              " 'ghost',\n",
              " 'backwards-down-the-number-line',\n",
              " 'harry-hood',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'run-like-an-antelope',\n",
              " 'set-e',\n",
              " 'character-zero',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'free',\n",
              " 'the-moma-dance',\n",
              " 'halleys-comet',\n",
              " 'stealing-time-from-the-faulty-plan',\n",
              " '555',\n",
              " 'rift',\n",
              " 'sample-in-a-jar',\n",
              " 'devotion-to-a-dream',\n",
              " 'yarmouth-road',\n",
              " 'sparkle',\n",
              " 'wingsuit',\n",
              " 'david-bowie',\n",
              " 'cavern',\n",
              " 'set-2',\n",
              " 'down-with-disease',\n",
              " 'whats-the-use',\n",
              " 'carini',\n",
              " 'light',\n",
              " 'fuego',\n",
              " 'slave-to-the-traffic-light',\n",
              " 'meatstick',\n",
              " 'bold-as-love',\n",
              " 'set-e',\n",
              " 'the-horse',\n",
              " 'silent-in-the-morning',\n",
              " 'fluffhead',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'acdc-bag',\n",
              " 'the-moma-dance',\n",
              " 'divided-sky',\n",
              " 'ocelot',\n",
              " 'funky-bitch',\n",
              " 'the-moma-dance',\n",
              " 'ocelot',\n",
              " 'stash',\n",
              " 'set-2',\n",
              " 'down-with-disease',\n",
              " 'prince-caspian',\n",
              " 'piper',\n",
              " 'twist',\n",
              " 'piper',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'run-like-an-antelope',\n",
              " 'set-e',\n",
              " 'loving-cup',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels)\n",
        "\n",
        "print(output_array.stack())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkzGors9Ydh6",
        "outputId": "6258865d-ec5c-4135-8743-98c09f9f313b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 62  25 110  27  11 197   2  85  49   9  51 167  10  42  80 108   8   3\n",
            " 228   4   1 126  35 152 238 209 186 134  79  83  94 182 133  19   2   7\n",
            " 198  70 131  89  93  21  92   3  52   4   1  30 170  16 182  54 136  87\n",
            " 209 110 267 219  19   2  84  79  51  49  88  21 108  15   3  41   4   1\n",
            "  48  56  82 133 152  33  31 261 223  27 197  17  11   2  28 158  85  93\n",
            " 131  44 143 165   3  66  60  52   4], shape=(99,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[ 62  25 110  27  11 197   2  85  49   9  51 167  10  42  80 108   8   3\n",
            " 228   4   1 126  35 152 238 209 186 134  79  83  94 182 133  19   2   7\n",
            " 198  70 131  89  93  21  92   3  52   4   1  30 170  16 182  54 136  87\n",
            " 209 110 267 219  19   2  84  79  51  49  88  21 108  15   3  41   4   1\n",
            "  48  56  82 133 152  33  31 261 223  27 197  17  11   2  28 158  85  93\n",
            " 131  44 143 165   3  66  60  52   4   0   1 103  56  23  45 127  16  13\n",
            "  16   2  28  65  57  70  57 108  21   3  77   4], shape=(120,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in labels[test_idx-1].numpy() if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1uDvSm5TIq8",
        "outputId": "519b680e-d741-4881-cee8-544dd9b5eda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set-e',\n",
              " 'bouncing-around-the-room',\n",
              " 'tweezer-reprise',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'acdc-bag',\n",
              " 'poor-heart',\n",
              " 'cities',\n",
              " 'kill-devil-falls',\n",
              " 'reba',\n",
              " 'possum',\n",
              " 'sample-in-a-jar',\n",
              " 'funky-bitch',\n",
              " 'maze',\n",
              " 'ocelot',\n",
              " 'sparkle',\n",
              " 'cavern',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'carini',\n",
              " 'ghost',\n",
              " 'mikes-song',\n",
              " 'simple',\n",
              " 'joy',\n",
              " 'weekapaug-groove',\n",
              " 'julius',\n",
              " 'sand',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'you-enjoy-myself',\n",
              " 'set-e',\n",
              " 'quinn-the-eskimo-the-mighty-quinn',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'my-soul',\n",
              " 'bathtub-gin',\n",
              " '555',\n",
              " 'pebbles-and-marbles',\n",
              " 'the-line',\n",
              " 'vultures',\n",
              " 'fast-enough-for-you',\n",
              " 'back-on-the-train',\n",
              " 'taste',\n",
              " 'gumbo',\n",
              " 'halfway-to-the-moon',\n",
              " 'stealing-time-from-the-faulty-plan',\n",
              " 'suzy-greenberg',\n",
              " 'set-2',\n",
              " 'chalk-dust-torture',\n",
              " 'scents-and-subtle-sounds',\n",
              " 'twist',\n",
              " 'fuego',\n",
              " 'the-wedge',\n",
              " 'light',\n",
              " 'harry-hood',\n",
              " 'first-tube',\n",
              " 'set-e',\n",
              " 'fluffhead',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'llama',\n",
              " 'undermind',\n",
              " 'stash',\n",
              " 'halfway-to-the-moon',\n",
              " 'i-didnt-know',\n",
              " 'nellie-kane',\n",
              " 'guyute',\n",
              " 'the-line',\n",
              " 'ocelot',\n",
              " 'no-quarter',\n",
              " 'ha-ha-ha',\n",
              " 'suzy-greenberg',\n",
              " 'set-2',\n",
              " '46-days',\n",
              " 'back-on-the-train',\n",
              " 'simple',\n",
              " 'ghost',\n",
              " 'backwards-down-the-number-line',\n",
              " 'harry-hood',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'run-like-an-antelope',\n",
              " 'set-e',\n",
              " 'character-zero',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'free',\n",
              " 'the-moma-dance',\n",
              " 'halleys-comet',\n",
              " 'stealing-time-from-the-faulty-plan',\n",
              " '555',\n",
              " 'rift',\n",
              " 'sample-in-a-jar',\n",
              " 'devotion-to-a-dream',\n",
              " 'yarmouth-road',\n",
              " 'sparkle',\n",
              " 'wingsuit',\n",
              " 'david-bowie',\n",
              " 'cavern',\n",
              " 'set-2',\n",
              " 'down-with-disease',\n",
              " 'whats-the-use',\n",
              " 'carini',\n",
              " 'light',\n",
              " 'fuego',\n",
              " 'slave-to-the-traffic-light',\n",
              " 'meatstick',\n",
              " 'bold-as-love',\n",
              " 'set-e',\n",
              " 'the-horse',\n",
              " 'silent-in-the-morning',\n",
              " 'fluffhead',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'the-curtain-with',\n",
              " 'wombat',\n",
              " 'kill-devil-falls',\n",
              " 'bouncing-around-the-room',\n",
              " 'poor-heart',\n",
              " 'a-song-i-heard-the-ocean-sing',\n",
              " 'lawn-boy',\n",
              " 'wolfmans-brother',\n",
              " 'waiting-all-night',\n",
              " 'winterqueen',\n",
              " 'funky-bitch',\n",
              " 'tube']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input, labels = val_data.take(1500)"
      ],
      "metadata": {
        "id": "sDH9j3INI488"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0][12:-13]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MIsw8P9RW8r",
        "outputId": "1c1f6d55-e4f1-4395-9cae-8f3c147fb11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 124), dtype=int64, numpy=\n",
              "array([[187, 106, 188, 300, 263,   3, 236,  92,   4,   1,  98, 118, 109,\n",
              "         45, 148, 325,  16, 194,  13, 111, 361,   2, 245, 281,  64, 176,\n",
              "        129,  21,   3,  47,  38,   4,   1,   9,  91, 144,  10, 180, 395,\n",
              "         89, 121, 378,  52,   2,  40,  14, 268,  57, 193,  70,  88,   3,\n",
              "        247,  26,   4,   1,  84, 206, 161, 124, 223,  34, 186, 238,  85,\n",
              "          2, 269, 190,   7, 300,  93, 218,   7,  18,  44,   3,  11, 107,\n",
              "         72,   4,   1, 172, 237, 172, 280, 137, 306,  49,  58, 170,  61,\n",
              "         17,   2, 188, 131, 128, 210, 181, 246,   8,   3, 165,   4,   1,\n",
              "         56, 147,  80, 231, 286,  20,  35,   2, 202, 282,  51, 213,  51,\n",
              "        143, 240,   6, 151, 313,  21,   3],\n",
              "       [211,   4,   1, 281,  21, 308,  24, 152, 235, 177, 210,  41,   2,\n",
              "        198, 218,  48, 151,  57,  49,   6, 231,  61, 154, 139,  22,   3,\n",
              "         80,   4,   1,  12,  82,  84,  44,  51, 239, 244, 183,  15,   2,\n",
              "         35, 188, 227,  89,  56,   3,  85,   4,   1, 120, 126,  79,  25,\n",
              "        161, 174, 287,  20, 325,   2, 336, 202, 197,   7,   3,  50,  45,\n",
              "          4,   1,  28,  18, 110,  33,  68,  58,  16, 140,   2,  70, 137,\n",
              "        218, 299,   6,  27,  37, 231,   3, 246,   4,   1, 278, 321, 124,\n",
              "        453,  23, 128,  29,   6,  29,   2,  43, 240, 282, 141,  88,   3,\n",
              "         38, 211,   4,   1,  42, 191, 245,  69,  13,  78, 207,  49,   2,\n",
              "         77,   9,  36,  10, 188, 247, 121],\n",
              "       [  3,  19,  11,   4,   1,  52,  74, 367,  61,  44,  30, 269,   2,\n",
              "        146,  48, 167,  80,  57, 198,   3, 282,  92,   4,   1, 265,   8,\n",
              "         56,  51, 217,  25,  47, 161,  15,   2, 130,  14,   6, 211, 158,\n",
              "         14,  86,   7, 122,  85, 278,  26,   3, 230,  35,   4,   1, 238,\n",
              "        119,  31,   6,  82, 208, 310, 202,   2, 137, 300,  84, 179,  10,\n",
              "        264, 148, 231,   3, 260, 143,  40, 247,   4,   1,  37, 415, 152,\n",
              "         84, 323, 193,   2, 181,   6,   6, 346, 328, 235,  46,  92,  41,\n",
              "          3,  88, 230,   4,   1, 317, 176,   9,  36,  10, 239, 263,   2,\n",
              "        262,  14,  62,  20,  80,  14, 231, 163,   3,  42,  26,   4,   1,\n",
              "        279, 110, 280,  15, 192, 256,  87],\n",
              "       [404, 166,  81,  30, 374,   2, 199, 258,  18,  57,  86, 153, 120,\n",
              "         40, 128,   3, 186, 112,  12,   4,   1,  71, 178,  49,  45, 103,\n",
              "         48,  17,   2,   6,   6, 454,   6, 426,   6,   6, 397,   6,   6,\n",
              "        301,   6, 169,  85, 282, 245, 314,  70, 246,   3,  21,   4,   1,\n",
              "        211,  49,  48,  79,  82, 211,   6,  37,   7,   2,  28,  88,  85,\n",
              "        190,   3, 108,  15,   4,   1, 100,  70, 258,  23, 310,  45,  11,\n",
              "          2, 202,  51, 198, 157, 107, 137,   3,  89, 112,  44,   4,   1,\n",
              "        446,   9,  36,  10,  84,  33, 148,  69, 426,  29,  19,   2, 146,\n",
              "        188,  65, 300, 141, 282,  21,   3, 247,  92,   4,   1,  85,  12,\n",
              "         56, 299, 149,  16, 137,   2, 231],\n",
              "       [301,  70, 208,  20, 246,   3,  94,  44,   4,   1,  19,  84, 218,\n",
              "         91, 144,  91,  45,  96,  49,   2,   7,  14,  37,  25, 264, 106,\n",
              "         21,  41,  14,  41,   3, 247,  26,   4,   1, 188,  61, 152,  79,\n",
              "        235, 176,  35, 211,   2, 202,  93, 131, 158,  88, 169,  48, 265,\n",
              "        171,  80,  29,   3,  39,   4,   1,  52,   9,  36,  10,  51, 454,\n",
              "         51,  23,  92,   2, 146,  65,  57, 111, 263, 282, 140,   3,  38,\n",
              "         17, 207,   4,   1,  70,  82,  80, 184, 191, 199, 182,  16, 239,\n",
              "        325,  35,   2,  30, 245,   6, 245,  49, 198,   7,  77,   3, 107,\n",
              "        211,   4,   1, 231,  12,  43,  20, 105, 189, 281,  48,   2, 265,\n",
              "        278, 163, 141, 158,  56, 148,  15]])>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(seed_sequence, max_length=25):\n",
        "    encoder_input = tf.constant([tokenizer.word_index[word] for word in seed_sequence], dtype=tf.int64)\n",
        "    encoder_input = tf.reshape(encoder_input, (1, len(encoder_input)))\n",
        "\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, tokenizer.word_index[seed_sequence[-1]])\n",
        "\n",
        "    unique_tokens = set([tokenizer.word_index[word] for word in seed_sequence])\n",
        "\n",
        "    i = 0\n",
        "    while i < max_length:\n",
        "        output = tf.reshape(output_array.stack(), (1, len(output_array.stack())))\n",
        "        predictions = gpt(output, training=False)\n",
        "\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)[0][0]\n",
        "\n",
        "        # Convert the scalar TensorFlow tensor to a tuple\n",
        "        predicted_id_tuple = (predicted_id.numpy(),)\n",
        "\n",
        "        # Check for duplicates\n",
        "        if predicted_id_tuple in unique_tokens :\n",
        "\n",
        "            output_array = output_array.write(i+1, predicted_id)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        unique_tokens.add(predicted_id_tuple)\n",
        "\n",
        "        output_array = output_array.write(i+1, predicted_id)\n",
        "\n",
        "        unique_tokens = set(unique_tokens)\n",
        "        #if predicted_id == 4:\n",
        "        #    output_array = output_array.write(i+1, predicted_id)\n",
        "        #    i += 1\n",
        "        #    continue;\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    predicted_sequence = [tokenizer.index_word[s] for s in output_array.stack().numpy()]\n",
        "\n",
        "    return predicted_sequence\n",
        "\n",
        "# Example usage:\n",
        "#org_list = corpus[-1].split()\n",
        "#org_list = ['sample-in-a-jar', 'sand', '555', 'rift', 'halfway-to-the-moon', 'horn', 'devotion-to-a-dream', 'blaze-on', 'tube', 'wolfmans-brother', 'set-2', 'ghost', 'birds-of-a-feather', 'mikes-song', 'the-wedge', 'fuego', 'shade', 'no-men-in-no-mans-land', 'weekapaug-groove', 'boogie-on-reggae-woman', 'chalk-dust-torture', 'set-e', 'theme-from-the-bottom', 'eos']\n",
        "org_list = ['farmhouse', 'first-tube', 'twist', 'divided-sky', 'ginseng-sullivan', 'carini', 'whats-the-use', 'wildcard', 'set-2', 'down-with-disease', 'the-moma-dance', 'piper', 'fee', 'gotta-jibboo', 'saw-it-again', 'split-open-and-melt', 'cavern', 'david-bowie', 'set-e', 'the-squirming-coil', 'eos']\n",
        "seed_sequence = org_list[:3]\n",
        "max_length = len(org_list) - len(seed_sequence)\n",
        "predicted_sequence = generate_sequence(seed_sequence, max_length=max_length)\n",
        "\n",
        "print(\"Original Sequence:\", org_list)\n",
        "print(\"Seed Sequence:\", seed_sequence)\n",
        "print(\"Predicted Sequence:\", (predicted_sequence[len(seed_sequence):]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6H-CkfqRbUg",
        "outputId": "d8081c32-bf8d-4221-894e-68af51398d0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sequence: ['farmhouse', 'first-tube', 'twist', 'divided-sky', 'ginseng-sullivan', 'carini', 'whats-the-use', 'wildcard', 'set-2', 'down-with-disease', 'the-moma-dance', 'piper', 'fee', 'gotta-jibboo', 'saw-it-again', 'split-open-and-melt', 'cavern', 'david-bowie', 'set-e', 'the-squirming-coil', 'eos']\n",
            "Seed Sequence: ['farmhouse', 'first-tube', 'twist']\n",
            "Predicted Sequence: ['set-e', 'loving-cup', 'eos', 'set-1', 'acdc-bag', 'the-moma-dance', 'runaway-jim', 'the-moma-dance', 'funky-bitch', 'the-moma-dance', 'funky-bitch', 'the-moma-dance', 'ocelot', 'set-2', 'down-with-disease', 'piper']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stopword_list = {}\n",
        "# Convert the lists to sets\n",
        "org_set = set(org_list)\n",
        "seed_set = set(seed_sequence)\n",
        "not_in_seed_sequence = org_set - seed_set\n",
        "filtered_songs = [song.strip() for song in not_in_seed_sequence if song not in stopword_list]\n",
        "predicted_sequence = [song.strip() for song in predicted_sequence[len(seed_sequence):] if song not in stopword_list]\n",
        "\n",
        "# Count the number of matching songs\n",
        "matching_songs = [song for song in filtered_songs if song in predicted_sequence]\n",
        "num_matching_songs = len(matching_songs)\n",
        "\n",
        "# Calculate the percentage of matching songs\n",
        "percentage_matching = (num_matching_songs / len(filtered_songs)) * 100\n",
        "\n",
        "print(\"Matching Songs in Predicted Sequence:\")\n",
        "print(matching_songs)\n",
        "print(\"Number of Matching Songs:\", num_matching_songs)\n",
        "print(\"Percentage of Matching Songs:\", percentage_matching)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34yNEGiNVjIK",
        "outputId": "c3f5a5d6-8e10-4143-c205-581e70757f78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching Songs in Predicted Sequence:\n",
            "['set-2', 'down-with-disease', 'set-e', 'piper', 'eos', 'the-moma-dance']\n",
            "Number of Matching Songs: 6\n",
            "Percentage of Matching Songs: 33.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1rPNlJ0WLw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
